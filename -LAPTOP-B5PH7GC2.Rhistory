show_file_path <- function(file_name) {
# List all R files in the package
r_files <- list.files(pattern = "\\.R$", recursive = TRUE, full.names = TRUE)
# Find the file that matches the given name
file_path <- r_files[grep(file_name, r_files)]
if (length(file_path) == 0) {
cat("File not found:", file_name, "\n")
} else if (length(file_path) > 1) {
cat("Multiple files found:\n")
cat(file_path, sep = "\n")
} else {
cat("File path:", file_path, "\n")
}
}
show_file_path("sentiment_analysis.R")
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::test()
testthat::test_file("tests/testthat/test-sentiment_analysis.R")
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::load_all(".")
library(here)
library(here)
getwd()
devtools::load_all(".")
getwd()
devtools::load_all(".")
devtools::load_all(".")
test_check("Goodreader")
devtools::load_all(".")
devtools::load_all(".")
getwd()
devtools::load_all(".")
devtools::load_all(".")
getwd()
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::load_all(".")
usethis::use_vignette()
usethis::use_vignette("Intro_to_Goodreader", title = "Introdution to Goodreader Package")
devtools::install()
warnings()
scrape_books <- function(book_ids_path, use_parallel = FALSE, num_cores = 4) {
book_ids <- readLines(book_ids_path)
scrape_book <- function(book_id) {
url <- paste0('https://www.goodreads.com/book/show/', book_id)
response <- readLines(url, warn = FALSE)
Sys.sleep(2)
get_content <- function(pattern) {
content <- grep(pattern, response, value = TRUE)
if (length(content) > 0) {
gsub(".*>(.*)<.*", "\\1", content[1])
} else {
NA
}
}
get_genres <- function() {
genres <- grep('class="Button--tag-inline"', response, value = TRUE)
paste(gsub(".*>(.*)<.*", "\\1", genres), collapse = ", ")
}
get_rating_distribution <- function() {
ratings <- grep('class="RatingsHistogram__bar"', response, value = TRUE)
distribution <- sapply(ratings, function(r) {
rating <- gsub('.*aria-label="([0-9]) star.*', "\\1", r)
count <- gsub('.*RatingsHistogram__labelTotal">([0-9,]+).*', "\\1", r)
paste(rating, gsub(",", "", count), sep = ":")
})
paste(distribution, collapse = ", ")
}
list(
book_id = book_id,
book_title = get_content('data-testid="bookTitle"'),
book_details = get_content('class="DetailsLayoutRightParagraph"'),
format = get_content('data-testid="pagesFormat"'),
publication_info = get_content('data-testid="publicationInfo"'),
authorlink = get_content('class="ContributorLink" href="([^"]+)"'),
author = get_content('class="ContributorLink__name">([^<]+)<'),
num_pages = get_content('data-testid="pagesFormat">([0-9]+)'),
genres = get_genres(),
num_ratings = get_content('data-testid="ratingsCount">([0-9,]+)'),
num_reviews = get_content('data-testid="reviewsCount">([0-9,]+)'),
average_rating = get_content('class="RatingStatistics__rating">([0-9.]+)<'),
rating_distribution = get_rating_distribution()
)
}
scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
do.call(rbind, lapply(book_data, as.data.frame))
}
scrape_books("D:\Downloads\book_ids20.txt")
scrape_books("D:\\Downloads\\book_ids20.txt")
library(parallel)
scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 4)
dd <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 4)
View(dd)
scrape_books <- function(book_ids_path, use_parallel = FALSE, num_cores = 4) {
book_ids <- readLines(book_ids_path)
scrape_book <- function(book_id) {
url <- paste0('https://www.goodreads.com/book/show/', book_id)
response <- httr::GET(url)
soup <- rvest::read_html(response)
Sys.sleep(2)
get_genres <- function(soup) {
genres_div <- rvest::html_node(soup, "[data-testid='genresList']")
if (!is.null(genres_div)) {
genres <- rvest::html_nodes(genres_div, "a.Button--tag-inline") %>%
rvest::html_text(trim = TRUE)
return(genres)
} else {
return(character(0))
}
}
get_rating_distribution <- function(soup) {
rating_bars <- rvest::html_nodes(soup, "div.RatingsHistogram__bar")
sapply(rating_bars, function(bar) {
rating <- rvest::html_attr(bar, "aria-label")
rating <- sub("^([0-9]) star.*", "\\1", rating)
num_ratings <- rvest::html_node(bar, "div.RatingsHistogram__labelTotal") %>%
rvest::html_text()
num_ratings <- gsub("[^0-9]", "", num_ratings)
paste(rating, num_ratings, sep = ":")
})
}
list(
book_id = book_id,
book_title = rvest::html_node(soup, "h1[data-testid='bookTitle']") %>%
rvest::html_text() %>%
gsub("\n", " ", x = .) %>%
trimws(),
book_details = rvest::html_node(soup, "div.DetailsLayoutRightParagraph") %>%
rvest::html_text(trim = TRUE),
format = rvest::html_node(soup, "p[data-testid='pagesFormat']") %>%
rvest::html_text(trim = TRUE),
publication_info = rvest::html_node(soup, "p[data-testid='publicationInfo']") %>%
rvest::html_text(trim = TRUE),
authorlink = rvest::html_node(soup, "a.ContributorLink") %>%
rvest::html_attr("href"),
author = rvest::html_node(soup, "span.ContributorLink__name") %>%
rvest::html_text(trim = TRUE),
num_pages = rvest::html_node(soup, "p[data-testid='pagesFormat']") %>%
rvest::html_text() %>%
sub(".*([0-9]+) pages.*", "\\1", x = .),
genres = paste(get_genres(soup), collapse = ", "),
num_ratings = rvest::html_node(soup, "span[data-testid='ratingsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
num_reviews = rvest::html_node(soup, "span[data-testid='reviewsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
average_rating = rvest::html_node(soup, "div.RatingStatistics__rating") %>%
rvest::html_text(trim = TRUE),
rating_distribution = paste(get_rating_distribution(soup), collapse = ", ")
)
}
scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
parallel::clusterEvalQ(cl, {
library(httr)
library(rvest)
})
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
do.call(rbind, lapply(book_data, as.data.frame))
}
dd <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
#' Scrape book details from Goodreads
#'
#' This function scrapes details of books using their IDs from Goodreads.
#'
#' @param book_ids_path Path to a text file containing book IDs.
#' @param use_parallel Logical indicating whether to scrape in parallel (default is FALSE).
#' @param num_cores Number of CPU cores to use for parallel scraping (default is 4).
#' @return A data frame containing scraped book details.
#' @importFrom httr GET
#' @importFrom rvest read_html html_node html_nodes html_text html_attr
#' @importFrom dplyr bind_rows
#' @importFrom magrittr %>%
#' @importFrom stringr str_extract
#' @importFrom parallel makeCluster clusterExport clusterEvalQ parLapply stopCluster
#' @importFrom rlang .data
#' @export
#' @examples
#' \dontrun{
#' scrape_books("book_ids.txt")
#' }
scrape_books <- function(book_ids_path, use_parallel = FALSE, num_cores = 4) {
book_ids <- readLines(book_ids_path)
scrape_book <- function(book_id) {
url <- paste0('https://www.goodreads.com/book/show/', book_id)
response <- httr::GET(url)
soup <- rvest::read_html(response)
Sys.sleep(2)
get_genres <- function(soup) {
genres_div <- soup %>% rvest::html_node("[data-testid='genresList']")
if (!is.null(genres_div)) {
genres <- genres_div %>%
rvest::html_nodes("a.Button--tag-inline") %>%
rvest::html_text(trim = TRUE)
return(genres)
} else {
return(character(0))
}
}
get_publication_info <- function(soup) {
soup %>%
rvest::html_nodes("div.FeaturedDetails") %>%
rvest::html_node("p[data-testid='publicationInfo']") %>%
rvest::html_text()
}
get_num_pages <- function(soup) {
featured_details <- soup %>% rvest::html_nodes("div.FeaturedDetails")
sapply(featured_details, function(detail) {
format_info <- detail %>% rvest::html_node("p[data-testid='pagesFormat']")
if (!is.null(format_info)) {
format_text <- format_info %>% rvest::html_text()
parts <- strsplit(format_text, ", ")[[1]]
if (length(parts) == 2) {
return(gsub("[^0-9]", "", parts[1]))
} else if (length(parts) == 1 && grepl("[0-9]", parts[1])) {
return(parts[1])
}
}
return(NA)
})
}
get_format_info <- function(soup) {
soup %>%
rvest::html_nodes("div.FeaturedDetails") %>%
rvest::html_node("p[data-testid='pagesFormat']") %>%
rvest::html_text()
}
get_rating_distribution <- function(soup) {
rating_bars <- soup %>% rvest::html_nodes("div.RatingsHistogram__bar")
sapply(rating_bars, function(bar) {
rating <- bar %>%
rvest::html_attr("aria-label") %>%
stringr::str_extract("^[0-9]")
num_ratings <- bar %>%
rvest::html_node("div.RatingsHistogram__labelTotal") %>%
rvest::html_text() %>%
stringr::str_extract("^[0-9,]+") %>%
gsub(",", "", x = .)
stats::setNames(num_ratings, rating)
})
}
book_details <- function(soup) {
tryCatch({
soup %>%
rvest::html_node("div.DetailsLayoutRightParagraph") %>%
rvest::html_text()
}, error = function(e) {
return(" ")
})
}
contributor_info <- function(soup) {
soup %>% rvest::html_node("a.ContributorLink")
}
list(
book_id = book_id,
book_title = soup %>%
rvest::html_node("h1[data-testid='bookTitle']") %>%
rvest::html_text() %>%
gsub("\n", " ", x = .) %>%
trimws(),
book_details = book_details(soup),
format = get_format_info(soup),
publication_info = get_publication_info(soup),
authorlink = contributor_info(soup) %>% rvest::html_attr("href"),
author = contributor_info(soup) %>%
rvest::html_node("span.ContributorLink__name") %>%
rvest::html_text(trim = TRUE),
num_pages = get_num_pages(soup),
genres = paste(get_genres(soup), collapse = ", "),
num_ratings = soup %>%
rvest::html_node("span[data-testid='ratingsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
num_reviews = soup %>%
rvest::html_node("span[data-testid='reviewsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
average_rating = soup %>%
rvest::html_node("div.RatingStatistics__rating") %>%
rvest::html_text(trim = TRUE),
rating_distribution = toString(get_rating_distribution(soup))
)
}
scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
parallel::clusterEvalQ(cl, {
library(httr)
library(rvest)
library(dplyr)
library(magrittr)
library(stringr)
library(stats)
})
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
dplyr::bind_rows(book_data)
}
ff <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
View(ff)
devtools::load_all(".")
install.packages('htmltools')
devtools::load_all(".")
rm(list = ls())
devtools::load_all(".")
devtools::load_all(".")
install.packages('xfun')
ls()
ls
rm(list = ls())
devtools::load_all(".")
devtools::load_all(".")
ff
dd <- search_goodreads(search_term = "parenting", search_in = "title", num_books = 10, sort_by = "ratings")
dd
class(dd)
gg <- scrape_books("D:\\Downloads\\book_ids20.txt")
names(gg)
class(gg)
gg <- preprocess_reviews(gg)
aa <- preprocess_reviews(gg)
names(gg)
names(dd)
hh <- scrape_reviews("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
aa <- preprocess_reviews(hh)
class(aa)
class(top_terms(aa))
bb <- fit_lda(aa)
bb <- fit_lda(aa$dtm, k = 5)
class(top_terms(bb))
kk <- top_terms(bb)
class(kk)
jj <- model_topics(hh)
class(jj)
class9bb
class(bb)
AIbooks <- search_goodreads(search_term = "artificial intelligence", search_in = "title", num_books = 100, sort_by = "ratings")
nrow(AIbooks)
get_book_ids(input_data = AIbooks, file_name = "D:\\Downloads\\AIbookids.txt") #the book IDs are now stored in a text file named “AIbookids”
bookinfo <- scrape_books(book_ids_path = "D:\\Downloads\\AIbookids.txt", use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
devtools::load_all(".")
bookreviews <- scrape_reviews(book_ids_path = "D:\\Downloads\\AIbookids.txt", num_reviews = 30, use_parallel = FALSE) #users can also turn on parallel process to speed up the process
devtools::load_all(".")
AIbooks <- search_goodreads(search_term = "artificial intelligence", search_in = "title", num_books = 100, sort_by = "ratings")
get_book_ids(input_data = AIbooks, file_name = "D:\\Downloads\\AIbookids.txt") #the book IDs are now stored in a text file named “AIbookids”
bookinfo <- scrape_books(book_ids_path = "D:\\Downloads\\AIbookids.txt", use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
bookreviews <- scrape_reviews(book_ids_path = "D:\\Downloads\\AIbookids.txt", num_reviews = 30, use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
sentiment_results <- analyze_sentiment(bookreviews, lexicon = "afinn")
average_sentiment <- average_book_sentiment(sentiment_results)
#' Create word cloud for topics
#'
#' This function creates a word cloud for each topic.
#'
#' @param model_output The output from model_topics function
#' @param n The number of top terms to include in the word cloud
#'
#' @importFrom wordcloud2 wordcloud2
#' @importFrom dplyr bind_rows
#' @export
gen_topic_clouds <- function(model_output, n = 50) {
lda_model <- model_output$model
beta <- exp(lda_model@beta)
terms <- lda_model@terms
wordcloud_data <- lapply(1:nrow(beta), function(i) {
topic_terms <- sort(beta[i,], decreasing = TRUE)
data.frame(word = terms[order(beta[i,], decreasing = TRUE)][1:n],
freq = sort(topic_terms, decreasing = TRUE)[1:n],
topic = i)
})
wordcloud_data <- dplyr::bind_rows(wordcloud_data)
# Create a list to store wordcloud plots
wordcloud_plots <- lapply(1:nrow(beta), function(topic) {
topic_data <- wordcloud_data[wordcloud_data$topic == topic, ]
wordcloud2::wordcloud2(topic_data[, c("word", "freq")], size = 0.5, shape = "circle")
})
return(wordcloud_plots)
}
average_book_sentiment(sentiment_results)
sentiment_histogram(sentiment_results)
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
devtools::load_all(".")
rm(list = c("gen_topic_clouds"))
devtools::load_all(".")
sentiment_trend(sentiment_results)
devtools::load_all(".")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
View(sentiment_results )
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
sentiment_trend(sentiment_results, show_smooth_trend = FALSE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE, time_period = "year")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, time_period = "year")
sentiment_trend(sentiment_results, time_period = "month")
sentiment_histogram(sentiment_results)
devtools::load_all(".")
sentiment_histogram(sentiment_results)
sentiment_trend(sentiment_results, time_period = "month")
sentiment_trend(sentiment_results, time_period = "year")
reviews_topic <- model_topics(bookreviews, num_topics = 3, num_terms = 10, english_only = TRUE)
devtools::load_all(".")
plot_topic_terms(reviews_topic)
plot_topic_prevalence(reviews_topic)
gen_topic_clouds(reviews_topic)
getwd
getwd()
setwd("D:\\Downloads")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
getwd()
View(sentiment_results)
parent_df <- search_goodreads(search_term = "parenting", search_in = "title", num_books = 10, sort_by = "ratings")
get_book_ids(input_data = parent_df, file_name = "parent_books.txt") #the book IDs are now stored in a text file named “parent_books”
parent_bookreviews <- scrape_reviews(book_ids_path = "parent_books.txt", num_reviews = 10, use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
sentiment_results <- analyze_sentiment(parent_bookreviews, lexicon = "afinn")
View(parent_bookreviews)
sentiment_trend(sentiment_results, time_period = "year")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
sentiment_trend(sentiment_results, time_period = "year")
summary(parent_df)
parent_df
head(parent_df)
gen_topic_clouds(reviews_topic)
reviews_topic <- model_topics(parent_bookreviews, num_topics = 3, num_terms = 10, english_only = TRUE)
gen_topic_clouds(reviews_topic)
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
usethis::use_readme_rmd()
devtools::load_all(".")
names(parent_df)
data.frame(title = c("Hamlet", "The Hunger Games", "Jane Eyre"),
author = c("William Shakespeare", "Suzanne Collins", "Charlotte Brontë")
book_id = c(1420, 2767052, 10210),
data.frame(title = c("Hamlet", "The Hunger Games", "Jane Eyre"),
author = c("William Shakespeare", "Suzanne Collins", "Charlotte Brontë"),
book_id = c(1420, 2767052, 10210),
url = c("https://www.goodreads.com/book/show/1420", "https://www.goodreads.com/book/show/2767052", "https://www.goodreads.com/book/show/10210"),
ratings = c(4.02, 4.34, 4.15)
)
temp_file <- tempfile(fileext = ".txt")
writeLines(c("1420", "2767052", "10210"), temp_file)
getwd()
file.remove(temp_file)
emp_file
temp_file
temp_file <- tempfile(fileext = ".txt")
writeLines(c("1420", "2767052", "10210"), temp_file)
file.remove(temp_file)
fj <- model_topics(parent_bookreviews)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
usethis::use_github_action()
devtools::load_all(".")
devtools::load_all(".")
