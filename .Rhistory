scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
do.call(rbind, lapply(book_data, as.data.frame))
}
scrape_books("D:\Downloads\book_ids20.txt")
scrape_books("D:\\Downloads\\book_ids20.txt")
library(parallel)
scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 4)
dd <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 4)
View(dd)
scrape_books <- function(book_ids_path, use_parallel = FALSE, num_cores = 4) {
book_ids <- readLines(book_ids_path)
scrape_book <- function(book_id) {
url <- paste0('https://www.goodreads.com/book/show/', book_id)
response <- httr::GET(url)
soup <- rvest::read_html(response)
Sys.sleep(2)
get_genres <- function(soup) {
genres_div <- rvest::html_node(soup, "[data-testid='genresList']")
if (!is.null(genres_div)) {
genres <- rvest::html_nodes(genres_div, "a.Button--tag-inline") %>%
rvest::html_text(trim = TRUE)
return(genres)
} else {
return(character(0))
}
}
get_rating_distribution <- function(soup) {
rating_bars <- rvest::html_nodes(soup, "div.RatingsHistogram__bar")
sapply(rating_bars, function(bar) {
rating <- rvest::html_attr(bar, "aria-label")
rating <- sub("^([0-9]) star.*", "\\1", rating)
num_ratings <- rvest::html_node(bar, "div.RatingsHistogram__labelTotal") %>%
rvest::html_text()
num_ratings <- gsub("[^0-9]", "", num_ratings)
paste(rating, num_ratings, sep = ":")
})
}
list(
book_id = book_id,
book_title = rvest::html_node(soup, "h1[data-testid='bookTitle']") %>%
rvest::html_text() %>%
gsub("\n", " ", x = .) %>%
trimws(),
book_details = rvest::html_node(soup, "div.DetailsLayoutRightParagraph") %>%
rvest::html_text(trim = TRUE),
format = rvest::html_node(soup, "p[data-testid='pagesFormat']") %>%
rvest::html_text(trim = TRUE),
publication_info = rvest::html_node(soup, "p[data-testid='publicationInfo']") %>%
rvest::html_text(trim = TRUE),
authorlink = rvest::html_node(soup, "a.ContributorLink") %>%
rvest::html_attr("href"),
author = rvest::html_node(soup, "span.ContributorLink__name") %>%
rvest::html_text(trim = TRUE),
num_pages = rvest::html_node(soup, "p[data-testid='pagesFormat']") %>%
rvest::html_text() %>%
sub(".*([0-9]+) pages.*", "\\1", x = .),
genres = paste(get_genres(soup), collapse = ", "),
num_ratings = rvest::html_node(soup, "span[data-testid='ratingsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
num_reviews = rvest::html_node(soup, "span[data-testid='reviewsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
average_rating = rvest::html_node(soup, "div.RatingStatistics__rating") %>%
rvest::html_text(trim = TRUE),
rating_distribution = paste(get_rating_distribution(soup), collapse = ", ")
)
}
scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
parallel::clusterEvalQ(cl, {
library(httr)
library(rvest)
})
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
do.call(rbind, lapply(book_data, as.data.frame))
}
dd <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
#' Scrape book details from Goodreads
#'
#' This function scrapes details of books using their IDs from Goodreads.
#'
#' @param book_ids_path Path to a text file containing book IDs.
#' @param use_parallel Logical indicating whether to scrape in parallel (default is FALSE).
#' @param num_cores Number of CPU cores to use for parallel scraping (default is 4).
#' @return A data frame containing scraped book details.
#' @importFrom httr GET
#' @importFrom rvest read_html html_node html_nodes html_text html_attr
#' @importFrom dplyr bind_rows
#' @importFrom magrittr %>%
#' @importFrom stringr str_extract
#' @importFrom parallel makeCluster clusterExport clusterEvalQ parLapply stopCluster
#' @importFrom rlang .data
#' @export
#' @examples
#' \dontrun{
#' scrape_books("book_ids.txt")
#' }
scrape_books <- function(book_ids_path, use_parallel = FALSE, num_cores = 4) {
book_ids <- readLines(book_ids_path)
scrape_book <- function(book_id) {
url <- paste0('https://www.goodreads.com/book/show/', book_id)
response <- httr::GET(url)
soup <- rvest::read_html(response)
Sys.sleep(2)
get_genres <- function(soup) {
genres_div <- soup %>% rvest::html_node("[data-testid='genresList']")
if (!is.null(genres_div)) {
genres <- genres_div %>%
rvest::html_nodes("a.Button--tag-inline") %>%
rvest::html_text(trim = TRUE)
return(genres)
} else {
return(character(0))
}
}
get_publication_info <- function(soup) {
soup %>%
rvest::html_nodes("div.FeaturedDetails") %>%
rvest::html_node("p[data-testid='publicationInfo']") %>%
rvest::html_text()
}
get_num_pages <- function(soup) {
featured_details <- soup %>% rvest::html_nodes("div.FeaturedDetails")
sapply(featured_details, function(detail) {
format_info <- detail %>% rvest::html_node("p[data-testid='pagesFormat']")
if (!is.null(format_info)) {
format_text <- format_info %>% rvest::html_text()
parts <- strsplit(format_text, ", ")[[1]]
if (length(parts) == 2) {
return(gsub("[^0-9]", "", parts[1]))
} else if (length(parts) == 1 && grepl("[0-9]", parts[1])) {
return(parts[1])
}
}
return(NA)
})
}
get_format_info <- function(soup) {
soup %>%
rvest::html_nodes("div.FeaturedDetails") %>%
rvest::html_node("p[data-testid='pagesFormat']") %>%
rvest::html_text()
}
get_rating_distribution <- function(soup) {
rating_bars <- soup %>% rvest::html_nodes("div.RatingsHistogram__bar")
sapply(rating_bars, function(bar) {
rating <- bar %>%
rvest::html_attr("aria-label") %>%
stringr::str_extract("^[0-9]")
num_ratings <- bar %>%
rvest::html_node("div.RatingsHistogram__labelTotal") %>%
rvest::html_text() %>%
stringr::str_extract("^[0-9,]+") %>%
gsub(",", "", x = .)
stats::setNames(num_ratings, rating)
})
}
book_details <- function(soup) {
tryCatch({
soup %>%
rvest::html_node("div.DetailsLayoutRightParagraph") %>%
rvest::html_text()
}, error = function(e) {
return(" ")
})
}
contributor_info <- function(soup) {
soup %>% rvest::html_node("a.ContributorLink")
}
list(
book_id = book_id,
book_title = soup %>%
rvest::html_node("h1[data-testid='bookTitle']") %>%
rvest::html_text() %>%
gsub("\n", " ", x = .) %>%
trimws(),
book_details = book_details(soup),
format = get_format_info(soup),
publication_info = get_publication_info(soup),
authorlink = contributor_info(soup) %>% rvest::html_attr("href"),
author = contributor_info(soup) %>%
rvest::html_node("span.ContributorLink__name") %>%
rvest::html_text(trim = TRUE),
num_pages = get_num_pages(soup),
genres = paste(get_genres(soup), collapse = ", "),
num_ratings = soup %>%
rvest::html_node("span[data-testid='ratingsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
num_reviews = soup %>%
rvest::html_node("span[data-testid='reviewsCount']") %>%
rvest::html_text() %>%
gsub("[^0-9]", "", x = .),
average_rating = soup %>%
rvest::html_node("div.RatingStatistics__rating") %>%
rvest::html_text(trim = TRUE),
rating_distribution = toString(get_rating_distribution(soup))
)
}
scrape_with_error_handling <- function(book_id) {
tryCatch(
scrape_book(book_id),
error = function(e) {
message(paste("Error scraping book ID:", book_id))
message(e)
return(NULL)
}
)
}
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
parallel::clusterExport(cl, c("scrape_book", "scrape_with_error_handling"), envir = environment())
parallel::clusterEvalQ(cl, {
library(httr)
library(rvest)
library(dplyr)
library(magrittr)
library(stringr)
library(stats)
})
book_data <- parallel::parLapply(cl, book_ids, scrape_with_error_handling)
parallel::stopCluster(cl)
} else {
book_data <- lapply(book_ids, scrape_with_error_handling)
}
book_data <- book_data[!sapply(book_data, is.null)]
dplyr::bind_rows(book_data)
}
ff <- scrape_books("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
View(ff)
devtools::load_all(".")
install.packages('htmltools')
devtools::load_all(".")
rm(list = ls())
devtools::load_all(".")
devtools::load_all(".")
install.packages('xfun')
ls()
ls
rm(list = ls())
devtools::load_all(".")
devtools::load_all(".")
ff
dd <- search_goodreads(search_term = "parenting", search_in = "title", num_books = 10, sort_by = "ratings")
dd
class(dd)
gg <- scrape_books("D:\\Downloads\\book_ids20.txt")
names(gg)
class(gg)
gg <- preprocess_reviews(gg)
aa <- preprocess_reviews(gg)
names(gg)
names(dd)
hh <- scrape_reviews("D:\\Downloads\\book_ids20.txt", use_parallel = TRUE, num_cores = 8)
aa <- preprocess_reviews(hh)
class(aa)
class(top_terms(aa))
bb <- fit_lda(aa)
bb <- fit_lda(aa$dtm, k = 5)
class(top_terms(bb))
kk <- top_terms(bb)
class(kk)
jj <- model_topics(hh)
class(jj)
class9bb
class(bb)
AIbooks <- search_goodreads(search_term = "artificial intelligence", search_in = "title", num_books = 100, sort_by = "ratings")
nrow(AIbooks)
get_book_ids(input_data = AIbooks, file_name = "D:\\Downloads\\AIbookids.txt") #the book IDs are now stored in a text file named “AIbookids”
bookinfo <- scrape_books(book_ids_path = "D:\\Downloads\\AIbookids.txt", use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
devtools::load_all(".")
bookreviews <- scrape_reviews(book_ids_path = "D:\\Downloads\\AIbookids.txt", num_reviews = 30, use_parallel = FALSE) #users can also turn on parallel process to speed up the process
devtools::load_all(".")
AIbooks <- search_goodreads(search_term = "artificial intelligence", search_in = "title", num_books = 100, sort_by = "ratings")
get_book_ids(input_data = AIbooks, file_name = "D:\\Downloads\\AIbookids.txt") #the book IDs are now stored in a text file named “AIbookids”
bookinfo <- scrape_books(book_ids_path = "D:\\Downloads\\AIbookids.txt", use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
bookreviews <- scrape_reviews(book_ids_path = "D:\\Downloads\\AIbookids.txt", num_reviews = 30, use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
sentiment_results <- analyze_sentiment(bookreviews, lexicon = "afinn")
average_sentiment <- average_book_sentiment(sentiment_results)
#' Create word cloud for topics
#'
#' This function creates a word cloud for each topic.
#'
#' @param model_output The output from model_topics function
#' @param n The number of top terms to include in the word cloud
#'
#' @importFrom wordcloud2 wordcloud2
#' @importFrom dplyr bind_rows
#' @export
gen_topic_clouds <- function(model_output, n = 50) {
lda_model <- model_output$model
beta <- exp(lda_model@beta)
terms <- lda_model@terms
wordcloud_data <- lapply(1:nrow(beta), function(i) {
topic_terms <- sort(beta[i,], decreasing = TRUE)
data.frame(word = terms[order(beta[i,], decreasing = TRUE)][1:n],
freq = sort(topic_terms, decreasing = TRUE)[1:n],
topic = i)
})
wordcloud_data <- dplyr::bind_rows(wordcloud_data)
# Create a list to store wordcloud plots
wordcloud_plots <- lapply(1:nrow(beta), function(topic) {
topic_data <- wordcloud_data[wordcloud_data$topic == topic, ]
wordcloud2::wordcloud2(topic_data[, c("word", "freq")], size = 0.5, shape = "circle")
})
return(wordcloud_plots)
}
average_book_sentiment(sentiment_results)
sentiment_histogram(sentiment_results)
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
devtools::load_all(".")
rm(list = c("gen_topic_clouds"))
devtools::load_all(".")
sentiment_trend(sentiment_results)
devtools::load_all(".")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
View(sentiment_results )
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE)
sentiment_trend(sentiment_results, show_smooth_trend = FALSE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = FALSE, time_period = "year")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, show_smooth_trend = TRUE, time_period = "year")
devtools::load_all(".")
sentiment_trend(sentiment_results, time_period = "year")
sentiment_trend(sentiment_results, time_period = "month")
sentiment_histogram(sentiment_results)
devtools::load_all(".")
sentiment_histogram(sentiment_results)
sentiment_trend(sentiment_results, time_period = "month")
sentiment_trend(sentiment_results, time_period = "year")
reviews_topic <- model_topics(bookreviews, num_topics = 3, num_terms = 10, english_only = TRUE)
devtools::load_all(".")
plot_topic_terms(reviews_topic)
plot_topic_prevalence(reviews_topic)
gen_topic_clouds(reviews_topic)
getwd
getwd()
setwd("D:\\Downloads")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
getwd()
View(sentiment_results)
parent_df <- search_goodreads(search_term = "parenting", search_in = "title", num_books = 10, sort_by = "ratings")
get_book_ids(input_data = parent_df, file_name = "parent_books.txt") #the book IDs are now stored in a text file named “parent_books”
parent_bookreviews <- scrape_reviews(book_ids_path = "parent_books.txt", num_reviews = 10, use_parallel = TRUE, num_cores = 8) #users can also turn on parallel process to speed up the process
sentiment_results <- analyze_sentiment(parent_bookreviews, lexicon = "afinn")
View(parent_bookreviews)
sentiment_trend(sentiment_results, time_period = "year")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::load_all("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
sentiment_trend(sentiment_results, time_period = "year")
summary(parent_df)
parent_df
head(parent_df)
gen_topic_clouds(reviews_topic)
reviews_topic <- model_topics(parent_bookreviews, num_topics = 3, num_terms = 10, english_only = TRUE)
gen_topic_clouds(reviews_topic)
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
usethis::use_readme_rmd()
devtools::load_all(".")
names(parent_df)
data.frame(title = c("Hamlet", "The Hunger Games", "Jane Eyre"),
author = c("William Shakespeare", "Suzanne Collins", "Charlotte Brontë")
book_id = c(1420, 2767052, 10210),
data.frame(title = c("Hamlet", "The Hunger Games", "Jane Eyre"),
author = c("William Shakespeare", "Suzanne Collins", "Charlotte Brontë"),
book_id = c(1420, 2767052, 10210),
url = c("https://www.goodreads.com/book/show/1420", "https://www.goodreads.com/book/show/2767052", "https://www.goodreads.com/book/show/10210"),
ratings = c(4.02, 4.34, 4.15)
)
temp_file <- tempfile(fileext = ".txt")
writeLines(c("1420", "2767052", "10210"), temp_file)
getwd()
file.remove(temp_file)
emp_file
temp_file
temp_file <- tempfile(fileext = ".txt")
writeLines(c("1420", "2767052", "10210"), temp_file)
file.remove(temp_file)
fj <- model_topics(parent_bookreviews)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
usethis::use_github_action()
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
library(flextable)
install.packages("flextable")
install.packages("flextable")
install.packages("flextable")
install.packages("flextable")
devtools::load_all(".")
use_github_action()
usethis::use_github_action()
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
usethis::use_git()
setwd("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader")
devtools::load_all(".")
usethis::use_github_action()
git remote add origin https://github.com/chaoliu-cl/Goodreader.git
usethis::git_remotes()
usethis::use_github_action()
usethis::use_git()  # Stage and commit changes
devtools::load_all(".")
usethis::use_git()  # Stage and commit changes
usethis::use_pkgdown()
pkgdown::build_site()
.Last.error
pkgdown::clean_site()
pkgdown::build_site()
devtools::load_all(".")
pkgdown::clean_site()
pkgdown::build_site()
devtools::load_all(".")
pkgdown::clean_site()
pkgdown::build_site()
pkgdown::clean_site()
pkgdown::build_site()
pkgdown::clean_site()
pkgdown::build_site()
devtools::load_all(".")
pkgdown::clean_site()
pkgdown::build_site()
pkgdown::clean_site()
pkgdown::build_site()
rcmdcheck::rcmdcheck(args = c("--as-cran"))
devtools::spell_check()
pkgbuild::build()
install.packages("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader_0.1.0.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/psych/OneDrive - Cedarville University/PYCH Thesis Research/goodreads_project/Goodreader_0.1.0.tar.gz", repos = NULL, type = "source")
devtools::load_all(".")
library(devtools)
usethis::use_release_issue()
usethis::use_release_issue()
usethis::use_news_md()
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
rcmdcheck::rcmdcheck(args = c("--as-cran"))
rcmdcheck::rcmdcheck(args = c("--as-cran"))
devtools::load_all(".")
devtools::build_vignettes()
devtools::build_vignettes()
usethis::::build_vignettes()
usethis::build_vignettes()
devtools::load_all(".")
rcmdcheck::rcmdcheck(args = c("--as-cran"))
library(kableExtra)
rcmdcheck::rcmdcheck(args = c("--as-cran"))
tools::check_urls_in_package_description()
devtools::check_urls_in_package_description()
install.packages("urlchecker")
install.packages("urlchecker")
urlchecker::url_check()
devtools::load_all(".")
devtools::load_all(".")
rcmdcheck::rcmdcheck(args = c("--as-cran"))
rcmdcheck::rcmdcheck(args = c("--as-cran"))
usethis::use_cran_comments()
R.version$version.string
R.version$platform
devtools::submit_cran()
