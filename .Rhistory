gap <- 2        # Maximum gap (adjust as needed)
regex <- NULL   # Regular expression (optional)
pthresh <- 0.05 # P-value threshold for significant patterns
# Fit the differential sequence mining function to the data
result <- differential_sequence_mining(left_group, right_group, sthresh, gap, regex, pthresh)
# Print the resulting patterns
print(result)
result
library(xml2)
library(httr)
library(rvest)
library(xml2)
library(dplyr)
# Function to fetch APA PsycNet search results
fetch_psycnet_results <- function(query, delay = 2) {
base_url <- "https://psycnet.apa.org/search?query="
url <- paste0(base_url, URLencode(query))
page <- httr::GET(url, httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"))
content <- httr::content(page, as = "text")
# Parse HTML content
parsed_html <- xml2::read_html(content)
# Extract necessary information
titles <- rvest::html_text(rvest::html_nodes(parsed_html, ".title-class"))
authors <- rvest::html_text(rvest::html_nodes(parsed_html, ".author-class"))
dates <- rvest::html_text(rvest::html_nodes(parsed_html, ".date-class"))
abstracts <- rvest::html_text(rvest::html_nodes(parsed_html, ".abstract-class"))
citations <- rvest::html_text(rvest::html_nodes(parsed_html, ".citation-class"))
journal_names <- rvest::html_text(rvest::html_nodes(parsed_html, ".journal-class"))
# Combine into a data frame
results <- data.frame(
Title = titles,
Authors = authors,
Date = dates,
Abstract = abstracts,
Citation = citations,
Journal = journal_names,
stringsAsFactors = FALSE
)
# Respectful delay
Sys.sleep(delay)
return(results)
}
fetch_psycnet_results("depression")
fetch_psycnet_results(depression)
#' Search PsycNet and extract paper information
#'
#' @param query The search query
#' @param max_results Maximum number of results to return
#' @return A data frame of paper information
#' @export
psycnet_search <- function(query, max_results = 100) {
# Construct the search URL
base_url <- "https://psycnet.apa.org/search/basic"
search_url <- paste0(base_url, "?query=", URLencode(query))
# Make the HTTP request
response <- httr::GET(search_url)
# Check for successful response
httr::stop_for_status(response)
# Parse the HTML content
content <- rvest::read_html(httr::content(response, "text"))
# Extract paper information (this part would need to be customized based on the website structure)
papers <- content %>%
rvest::html_nodes(".result-item") %>%
rvest::html_text() %>%
strsplit("\n") %>%
lapply(function(x) {
list(
title = x[1],
authors = x[2],
journal = x[3],
year = as.numeric(gsub(".*([0-9]{4}).*", "\\1", x[3]))
)
})
# Convert to data frame
result <- do.call(rbind, lapply(papers[1:min(max_results, length(papers))], as.data.frame))
return(result)
}
psycnet_search(depression)
psycnet_search("depression")
#' Search PsycNet and extract paper information
#'
#' @param query The search query
#' @param max_results Maximum number of results to return
#' @return A data frame of paper information
#' @export
psycnet_search <- function(query, max_results = 10) {
# Construct the search URL
base_url <- "https://psycnet.apa.org/search/basic"
search_url <- paste0(base_url, "?query=", URLencode(query))
# Make the HTTP request
response <- httr::GET(search_url)
# Check for successful response
httr::stop_for_status(response)
# Parse the HTML content
content <- rvest::read_html(httr::content(response, "text"))
# Extract paper information (this part would need to be customized based on the website structure)
papers <- content %>%
rvest::html_nodes(".result-item") %>%
rvest::html_text() %>%
strsplit("\n") %>%
lapply(function(x) {
list(
title = x[1],
authors = x[2],
journal = x[3],
year = as.numeric(gsub(".*([0-9]{4}).*", "\\1", x[3]))
)
})
# Convert to data frame
result <- do.call(rbind, lapply(papers[1:min(max_results, length(papers))], as.data.frame))
return(result)
}
psycnet_search("depression")
#' Authenticate with PsycNet
#'
#' @param username Your PsycNet username
#' @param password Your PsycNet password
#' @return A session object
#' @export
psycnet_login <- function(username, password) {
login_url <- "https://psycnet.apa.org/login"
# Create a session object
session <- httr::handle_reset(login_url)
# Perform login
response <- httr::POST(login_url,
handle = session,
body = list(username = username, password = password),
encode = "form")
httr::stop_for_status(response)
return(session)
}
#' Search PsycNet and extract paper information
#'
#' @param query The search query
#' @param max_results Maximum number of results to return
#' @param session A session object from psycnet_login()
#' @return A data frame of paper information
#' @export
psycnet_search <- function(query, max_results = 10, session) {
# Construct the search URL
base_url <- "https://psycnet.apa.org/search/basic"
search_url <- paste0(base_url, "?query=", URLencode(query))
# Make the HTTP request using the authenticated session
response <- httr::GET(search_url, handle = session)
# Check for successful response
httr::stop_for_status(response)
# Parse the HTML content
content <- rvest::read_html(httr::content(response, "text"))
# Extract paper information (this part would need to be customized based on the website structure)
papers <- content %>%
rvest::html_nodes(".result-item") %>%
rvest::html_text() %>%
strsplit("\n") %>%
lapply(function(x) {
list(
title = x[1],
authors = x[2],
journal = x[3],
year = as.numeric(gsub(".*([0-9]{4}).*", "\\1", x[3]))
)
})
# Convert to data frame
result <- do.call(rbind, lapply(papers[1:min(max_results, length(papers))], as.data.frame))
return(result)
}
# Usage example:
# session <- psycnet_login("your_username", "your_password")
# results <- psycnet_search("depression", max_results = 10, session = session)
session <- psycnet_login("psychaoliu@gmail.com", "83730655Lc!")
session <- psycnet_login(psychaoliu@gmail.com, 83730655Lc!)
#' Scrape book reviews from Goodreads
#'
#' This function scrapes book reviews from Goodreads based on provided book IDs.
#'
#' @param book_ids_path A character string specifying the path to a file containing book IDs.
#' @param num_reviews An integer specifying the number of reviews to scrape per book. Default is 30.
#' @param use_parallel A logical value indicating whether to use parallel processing. Default is FALSE.
#' @param num_cores An integer specifying the number of cores to use for parallel processing. Default is 4.
#'
#' @return A data frame containing scraped review information.
#'
#' @import rvest dplyr stringr purrr parallel httr
#' @importFrom rlang .data
#' @export
#'
#' @examples
#' \dontrun{
#' reviews <- scrape_reviews("book_ids.txt", num_reviews = 20)
#' }
scrape_reviews <- function(book_ids_path, num_reviews = 30, use_parallel = FALSE, num_cores = 4) {
start_time <- Sys.time()
script_name <- "scrape_goodreads_reviews"
# Utility functions
get_user_id <- function(article) {
avatar <- article %>% rvest::html_node('.ReviewerProfile__avatar a')
user_id_link <- rvest::html_attr(avatar, 'href')
stringr::str_extract(user_id_link, "\\d+")
}
get_rating_and_date_user <- function(article) {
rating_date_user <- article %>% rvest::html_node('.ReviewCard__row')
date_element <- rating_date_user %>% rvest::html_node('.Text__body3 a')
review_date <- rvest::html_text(date_element, trim = TRUE)
rating_element <- rating_date_user %>% rvest::html_node('.RatingStars__small')
aria_label <- rvest::html_attr(rating_element, 'aria-label')
rating <- ifelse(!is.null(aria_label), stringr::str_extract(aria_label, "\\d+"), NA)
list(review_date = review_date, rating = rating)
}
get_reviewers_info <- function(review_articles, book_id) {
purrr::map(review_articles, function(article) {
review_content <- article %>% rvest::html_node('.ReviewText__content') %>% rvest::html_text(trim = TRUE)
rating_date <- get_rating_and_date_user(article)
list(
book_id = book_id,
reviewer_id = tryCatch(get_user_id(article), error = function(e) NA),
reviewer_name = tryCatch(article %>% rvest::html_node('.ReviewerProfile__name') %>% rvest::html_text(trim = TRUE), error = function(e) NA),
review_content = review_content,
reviewer_followers = tryCatch(article %>% rvest::html_nodes('.ReviewerProfile__meta span') %>% `[`(2) %>% rvest::html_text(trim = TRUE) %>% stringr::str_extract("\\d+") %>% as.numeric(), error = function(e) NA),
reviewer_total_reviews = tryCatch(article %>% rvest::html_nodes('.ReviewerProfile__meta span') %>% `[`(1) %>% rvest::html_text(trim = TRUE) %>% stringr::str_extract("\\d+") %>% as.numeric(), error = function(e) NA),
review_date = rating_date$review_date,
review_rating = as.numeric(rating_date$rating)
)
})
}
# Function to get random User-Agent
get_random_user_agent <- function() {
user_agents <- c(
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
)
sample(user_agents, 1)
}
# Function to get random delay
get_random_delay <- function(min_delay = 1, max_delay = 5) {
runif(1, min = min_delay, max = max_delay)
}
scrape_reviews_for_book <- function(book_id, num_reviews) {
result_list <- vector("list", num_reviews)
tryCatch({
url <- paste0("https://www.goodreads.com/book/show/", book_id)
# Initialize session with random User-Agent
session <- rvest::session(url, httr::user_agent(get_random_user_agent()))
# Throttle requests
Sys.sleep(get_random_delay())
reviews_collected <- 0
more_reviews_available <- TRUE
page_number <- 1
while (more_reviews_available && reviews_collected < num_reviews) {
page <- session$response %>% rvest::read_html()
review_articles <- page %>% rvest::html_nodes('article.ReviewCard')
results <- get_reviewers_info(review_articles, book_id)
if (length(results) > 0) {
new_reviews <- min(length(results), num_reviews - reviews_collected)
result_list[(reviews_collected + 1):(reviews_collected + new_reviews)] <- results[1:new_reviews]
reviews_collected <- reviews_collected + new_reviews
}
if (reviews_collected >= num_reviews) {
break
}
more_button <- page %>% rvest::html_node('button.Button.Button--secondary[data-testid="pagination-button-next"]')
if (!is.na(more_button)) {
onclick_attr <- rvest::html_attr(more_button, "onclick")
if (!is.null(onclick_attr)) {
more_url <- stringr::str_extract(onclick_attr, "https://[^']+")
if (!is.null(more_url)) {
# Update session with new random User-Agent for each page
session <- rvest::session_jump_to(session, more_url, httr::user_agent(get_random_user_agent()))
# Throttle requests
Sys.sleep(get_random_delay())
page_number <- page_number + 1
} else {
more_reviews_available <- FALSE
}
} else {
more_reviews_available <- FALSE
}
} else {
more_reviews_available <- FALSE
}
}
}, error = function(e) {
message(sprintf("Error processing book ID %s: %s", book_id, e$message))
})
if (length(result_list) == 0) {
result_list <- list(list(
book_id = book_id,
reviewer_id = NA,
reviewer_name = NA,
review_content = NA,
reviewer_followers = NA,
reviewer_total_reviews = NA,
review_date = NA,
review_rating = NA
))
}
result_list
}
# Read book IDs
book_ids <- readLines(book_ids_path)
message(sprintf("Total book IDs to process: %d", length(book_ids)))
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
# Export the entire environment of the scrape_reviews function
environment_to_export <- environment()
parallel::clusterExport(cl, varlist = ls(envir = environment_to_export), envir = environment_to_export)
parallel::clusterEvalQ(cl, {
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(httr)
})
all_reviews <- parallel::parLapply(cl, book_ids, function(book_id) {
scrape_reviews_for_book(book_id, num_reviews)
})
parallel::stopCluster(cl)
} else {
all_reviews <- lapply(book_ids, function(book_id) scrape_reviews_for_book(book_id, num_reviews))
}
all_reviews_df <- dplyr::bind_rows(all_reviews)
message(sprintf("%s %s: Completed! All book reviews extracted", Sys.time(), script_name))
message(sprintf("Scraping run time = %s", Sys.time() - start_time))
message(sprintf("Total books processed: %d", length(unique(all_reviews_df$book_id))))
all_reviews_df
}
reviews <- scrape_reviews("C:\\Users\\chaoliu\\Downloads\\book_ids20.txt", num_reviews = 50, use_parallel = TRUE, num_cores = 8)
View(reviews)
#' Scrape book reviews from Goodreads
#'
#' This function scrapes book reviews from Goodreads based on provided book IDs.
#'
#' @param book_ids_path A character string specifying the path to a file containing book IDs.
#' @param num_reviews An integer specifying the number of reviews to scrape per book. Default is 30.
#' @param use_parallel A logical value indicating whether to use parallel processing. Default is FALSE.
#' @param num_cores An integer specifying the number of cores to use for parallel processing. Default is 4.
#'
#' @return A data frame containing scraped review information.
#'
#' @import rvest dplyr stringr purrr parallel httr
#' @importFrom rlang .data
#' @export
#'
#' @examples
#' \dontrun{
#' reviews <- scrape_reviews("book_ids.txt", num_reviews = 20)
#' }
scrape_reviews <- function(book_ids_path, num_reviews = 30, use_parallel = FALSE, num_cores = 4) {
start_time <- Sys.time()
script_name <- "scrape_goodreads_reviews"
# Utility functions
get_user_id <- function(article) {
avatar <- article %>% rvest::html_node('.ReviewerProfile__avatar a')
user_id_link <- rvest::html_attr(avatar, 'href')
stringr::str_extract(user_id_link, "\\d+")
}
get_rating_and_date_user <- function(article) {
rating_date_user <- article %>% rvest::html_node('.ReviewCard__row')
date_element <- rating_date_user %>% rvest::html_node('.Text__body3 a')
review_date <- rvest::html_text(date_element, trim = TRUE)
rating_element <- rating_date_user %>% rvest::html_node('.RatingStars__small')
aria_label <- rvest::html_attr(rating_element, 'aria-label')
rating <- ifelse(!is.null(aria_label), stringr::str_extract(aria_label, "\\d+"), NA)
list(review_date = review_date, rating = rating)
}
get_reviewers_info <- function(review_articles, book_id) {
purrr::map(review_articles, function(article) {
review_content <- article %>% rvest::html_node('.ReviewText__content') %>% rvest::html_text(trim = TRUE)
rating_date <- get_rating_and_date_user(article)
list(
book_id = book_id,
reviewer_id = tryCatch(get_user_id(article), error = function(e) NA),
reviewer_name = tryCatch(article %>% rvest::html_node('.ReviewerProfile__name') %>% rvest::html_text(trim = TRUE), error = function(e) NA),
review_content = review_content,
reviewer_followers = tryCatch(article %>% rvest::html_nodes('.ReviewerProfile__meta span') %>% `[`(2) %>% rvest::html_text(trim = TRUE) %>% stringr::str_extract("\\d+") %>% as.numeric(), error = function(e) NA),
reviewer_total_reviews = tryCatch(article %>% rvest::html_nodes('.ReviewerProfile__meta span') %>% `[`(1) %>% rvest::html_text(trim = TRUE) %>% stringr::str_extract("\\d+") %>% as.numeric(), error = function(e) NA),
review_date = rating_date$review_date,
review_rating = as.numeric(rating_date$rating)
)
})
}
# Function to generate random User-Agent
get_random_user_agent <- function() {
user_agents <- c(
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36"
)
sample(user_agents, 1)
}
# Function to get a random delay
get_random_delay <- function(min_delay = 2, max_delay = 5) {
runif(1, min_delay, max_delay)
}
# Function to rotate IP (placeholder - implement your own IP rotation logic)
rotate_ip <- function() {
# Implement your IP rotation logic here
# This could involve using a VPN service or a list of proxy servers
message("Rotating IP address...")
}
scrape_reviews_for_book <- function(book_id, num_reviews) {
result_list <- vector("list", num_reviews)
tryCatch({
url <- paste0("https://www.goodreads.com/book/show/", book_id)
# Create a new session with custom headers
session <- rvest::session(url, httr::add_headers(
"User-Agent" = get_random_user_agent(),
"Accept-Language" = "en-US,en;q=0.5",
"Accept-Encoding" = "gzip, deflate, br",
"Connection" = "keep-alive",
"Upgrade-Insecure-Requests" = "1"
))
# Implement delay
Sys.sleep(get_random_delay())
reviews_collected <- 0
more_reviews_available <- TRUE
page_number <- 1
while (more_reviews_available && reviews_collected < num_reviews) {
page <- session$response %>% rvest::read_html()
review_articles <- page %>% rvest::html_nodes('article.ReviewCard')
results <- get_reviewers_info(review_articles, book_id)
if (length(results) > 0) {
new_reviews <- min(length(results), num_reviews - reviews_collected)
result_list[(reviews_collected + 1):(reviews_collected + new_reviews)] <- results[1:new_reviews]
reviews_collected <- reviews_collected + new_reviews
}
if (reviews_collected >= num_reviews) {
break
}
more_button <- page %>% rvest::html_node('button.Button.Button--secondary[data-testid="pagination-button-next"]')
if (!is.na(more_button)) {
onclick_attr <- rvest::html_attr(more_button, "onclick")
if (!is.null(onclick_attr)) {
more_url <- stringr::str_extract(onclick_attr, "https://[^']+")
if (!is.null(more_url)) {
# Update session with new headers for each request
session <- rvest::session_jump_to(session, more_url, httr::add_headers(
"User-Agent" = get_random_user_agent(),
"Accept-Language" = "en-US,en;q=0.5",
"Accept-Encoding" = "gzip, deflate, br",
"Connection" = "keep-alive",
"Upgrade-Insecure-Requests" = "1"
))
# Implement delay between requests
Sys.sleep(get_random_delay())
page_number <- page_number + 1
# Rotate IP after every 5 pages
if (page_number %% 5 == 0) {
rotate_ip()
}
} else {
more_reviews_available <- FALSE
}
} else {
more_reviews_available <- FALSE
}
} else {
more_reviews_available <- FALSE
}
}
}, error = function(e) {
message(sprintf("Error processing book ID %s: %s", book_id, e$message))
})
if (length(result_list) == 0) {
result_list <- list(list(
book_id = book_id,
reviewer_id = NA,
reviewer_name = NA,
review_content = NA,
reviewer_followers = NA,
reviewer_total_reviews = NA,
review_date = NA,
review_rating = NA
))
}
result_list
}
# Read book IDs
book_ids <- readLines(book_ids_path)
message(sprintf("Total book IDs to process: %d", length(book_ids)))
if (use_parallel) {
cl <- parallel::makeCluster(num_cores)
# Export the entire environment of the scrape_reviews function
environment_to_export <- environment()
parallel::clusterExport(cl, varlist = ls(envir = environment_to_export), envir = environment_to_export)
parallel::clusterEvalQ(cl, {
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(httr)
})
all_reviews <- parallel::parLapply(cl, book_ids, function(book_id) {
scrape_reviews_for_book(book_id, num_reviews)
})
parallel::stopCluster(cl)
} else {
all_reviews <- lapply(book_ids, function(book_id) scrape_reviews_for_book(book_id, num_reviews))
}
all_reviews_df <- dplyr::bind_rows(all_reviews)
message(sprintf("%s %s: Completed! All book reviews extracted", Sys.time(), script_name))
message(sprintf("Scraping run time = %s", Sys.time() - start_time))
message(sprintf("Total books processed: %d", length(unique(all_reviews_df$book_id))))
all_reviews_df
}
reviews <- scrape_reviews("C:\\Users\\chaoliu\\Downloads\\book_ids20.txt", num_reviews = 50, use_parallel = TRUE, num_cores = 8)
install.packages("woslite_r_client")
install_github("Clarivate-SAR/woslite_r_client")
library(devtools)
install_github("Clarivate-SAR/woslite_r_client")
library(woslite_r_client)
library(wosliterclient)
ApiClient()
